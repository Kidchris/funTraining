{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Natural Language Processing For Predicting Sentiment analysis on IMDB dataset Loading from Local\n",
    "- The model Developed is only based on Embedding Layer, Conv1D, MaxPooling1D and Dense Layers\n",
    "- The processing methods are simply processing methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "import numpy as np\n",
    "import nltk\n",
    "from os import listdir\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def opener(file_name):\n",
    "    text = open(file_name)\n",
    "    content = text.read()\n",
    "    text.close()\n",
    "    return content\n",
    "\n",
    "def preprocess(text : str):\n",
    "    tokens = text.split()\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens if (w not in stop_words and len(w)>1 and w.isalpha())]\n",
    "    return tokens\n",
    "\n",
    "def update_vocab(file_name, vocab):\n",
    "    text = opener(file_name)\n",
    "    tokens = preprocess(text)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "def put_all_together(directory):\n",
    "    doc = []\n",
    "    for file in listdir(directory):\n",
    "        if not file.startswith(\"cv9\"):\n",
    "            path = directory + \"/\" + file\n",
    "            content = opener(path)\n",
    "            tokens = preprocess(content)\n",
    "            doc.append(tokens)\n",
    "    return doc\n",
    "def put_all_together_test(directory):\n",
    "    doc = []\n",
    "    for file in listdir(directory):\n",
    "        if file.startswith(\"cv9\"):\n",
    "            path = directory + \"/\" + file\n",
    "            content = opener(path)\n",
    "            tokens = preprocess(content)\n",
    "            doc.append(tokens)\n",
    "    return doc\n",
    "            \n",
    "def save_tokens_to_file(directory, vocab, min_length=2):\n",
    "    tokens = [k for k,v in vocab.items() if v>min_length]\n",
    "    l = \"\\n\".join(sorted(tokens))\n",
    "    f = open(directory, \"w\")\n",
    "    f.write((l))\n",
    "    f.close()\n",
    "\n",
    "pos = put_all_together(\"./txt_sentoken/pos\")\n",
    "neg = put_all_together(\"./txt_sentoken/neg\")\n",
    "train_set = pos + neg\n",
    "y_train = [1 for _ in range(900)]+[0 for _ in range(900)]\n",
    "pos_test = put_all_together_test(\"./txt_sentoken/pos\")\n",
    "neg_test = put_all_together_test(\"./txt_sentoken/neg\")\n",
    "test_set = pos_test + neg_test\n",
    "y_test = [1 for _ in range(len(pos_test))] + [0 for _ in range(len(neg_test))]\n",
    "\n",
    "m_ = max([ len(i) for i in train_set])\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_set)\n",
    "encoded_senteces = tokenizer.texts_to_sequences(train_set)\n",
    "padded_sequences = pad_sequences(encoded_senteces, padding=\"post\")\n",
    "test_set = tokenizer.texts_to_sequences(test_set)\n",
    "test_sequences = pad_sequences(test_set, maxlen=m_, padding=\"post\")\n",
    "y_test = np.array(y_test)\n",
    "v_size = len(tokenizer.word_index) + 1;\n",
    "model = Sequential()\n",
    "model.add(Embedding(v_size, 100, input_length= m_))\n",
    "model.add(Conv1D(32, kernel_size=3, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation=\"relu\", kernel_regularizer=\"l2\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "res = model.fit(padded_sequences, y_train, epochs=10, validation_data=(test_sequences, y_test))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(res.history[\"accuracy\"], label='Acc')\n",
    "plt.plot(res.history[\"val_accuracy\"], label='Validation Acc')\n",
    "plt.plot(res.history[\"loss\"], label='Loss')\n",
    "plt.plot(res.history[\"val_loss\"], label='Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "sent = [[\"worst\" , \"moovie\" , \"ever\", \"seen\"]]\n",
    "t = tokenizer.texts_to_sequences(sent)\n",
    "p = pad_sequences(t, maxlen=m_, padding=\"post\")\n",
    "np.argmax(model.predict(p), axis=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## This is a vectorization of words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "\tclean_lines = list()\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\tlines = doc.splitlines()\n",
    "\tfor line in lines:\n",
    "\t\ttokens = line.split()\n",
    "\t\ttokens = [w.translate(table) for w in tokens]\n",
    "\t\ttokens = [w for w in tokens if w in vocab]\n",
    "\t\tclean_lines.append(tokens)\n",
    "\treturn clean_lines\n",
    "\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "\tlines = list()\n",
    "\tfor filename in listdir(directory):\n",
    "\t\tif is_trian and filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_trian and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\tdoc = load_doc(path)\n",
    "\t\tdoc_lines = doc_to_clean_lines(doc, vocab)\n",
    "\t\tlines += doc_lines\n",
    "\treturn lines\n",
    "\n",
    "vocab_filename = 'tokens.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "positive_docs = process_docs('txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_docs('txt_sentoken/neg', vocab, True)\n",
    "sentences = negative_docs + positive_docs\n",
    "print('Total training sentences: %d' % len(sentences))\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, workers=8, min_count=1)\n",
    "words = list(model.wv.index_to_key)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total training sentences: 58109\n",
      "Vocabulary size: 18112\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "61f16b98c194273433d28da8ad4eebd31a866a26d2fd43b3ba83f196243e57b5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}